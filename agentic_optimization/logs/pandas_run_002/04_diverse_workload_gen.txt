╔══════════════════════════════════════════════════════════════════════════════════════════════════╗
║                                                                                                  ║
║                                       DIVERSE WORKLOAD GEN                                       ║
║                                                                                                  ║
╚══════════════════════════════════════════════════════════════════════════════════════════════════╝

Timestamp: 2026-02-13 23:57:03
Run Directory: logs/pandas_run_002




====================================================================================================
DIVERSE WORKLOAD GENERATION (PHASE 2)====================================================================================================

Started: 2026-02-13 23:57:03



====================================================================================================
GENERATED DIVERSE WORKLOADS====================================================================================================

Total workloads generated: 3



────────────────────────────────────────────────────────────────────────────────────────────────────
WORKLOAD 1: diverse_1
────────────────────────────────────────────────────────────────────────────────────────────────────
Description: This workload tests the worst-case scenario by concatenating large and varied DataFrame objects to explore worst-case execution paths.

Code:
import pandas as pd
import numpy as np
import timeit
import statistics

def setup():
    '''Setup - create test data targeting worst-case scenario'''
    global df_large, df_varied
    
    # Create large and varied DataFrame objects
    df_large = pd.DataFrame(np.random.rand(100000, 10))
    df_varied = pd.DataFrame(np.random.randint(0, 100, size=(100000, 10)))

def workload():
    '''Workload - execute concatenation of DataFrames'''
    global df_large, df_varied
    
    result = pd.concat([df_large, df_varied])

# Benchmark (DO NOT MODIFY)
runtimes = timeit.repeat(workload, number=1, repeat=3, setup=setup)
print(f"Mean: {statistics.mean(runtimes):.6f}")
print(f"Std Dev: {statistics.stdev(runtimes):.6f}")




────────────────────────────────────────────────────────────────────────────────────────────────────
WORKLOAD 2: diverse_2
────────────────────────────────────────────────────────────────────────────────────────────────────
Description: This workload targets edge cases in merging operations of DataFrames, such as merging with conflicting indices or containing null values.

Code:
import pandas as pd
import numpy as np
import timeit
import statistics

def setup():
    '''Setup - create test data targeting edge cases'''
    global df1, df2
    
    # Create DataFrames with conflicting indices and null values
    df1 = pd.DataFrame({'A': [1, 2, np.nan], 'B': ['x', 'y', 'z']}, index=[0, 1, 2])
    df2 = pd.DataFrame({'A': [4, np.nan, 6], 'C': ['u', 'v', 'w']}, index=[2, 3, 4])

def workload():
    '''Workload - execute merge on DataFrames with edge cases'''
    global df1, df2
    
    result = pd.merge(df1, df2, on='A', how='outer')

# Benchmark (DO NOT MODIFY)
runtimes = timeit.repeat(workload, number=1, repeat=3, setup=setup)
print(f"Mean: {statistics.mean(runtimes):.6f}")
print(f"Std Dev: {statistics.stdev(runtimes):.6f}")




────────────────────────────────────────────────────────────────────────────────────────────────────
WORKLOAD 3: diverse_3
────────────────────────────────────────────────────────────────────────────────────────────────────
Description: This workload stresses the memory management by creating and processing large tiled arrays, targeting memory allocation and processing branches.

Code:
import pandas as pd
import numpy as np
import timeit
import statistics

def setup():
    '''Setup - create test data targeting memory pressure'''
    global large_array
    
    # Create a large tiled array to stress memory allocation
    small_array = np.random.rand(100, 100)
    large_array = np.tile(small_array, (1000, 1000))

def workload():
    '''Workload - execute operation on large tiled array'''
    global large_array
    
    # Mimic an operation that processes the large array
    result = np.sum(large_array)

# Benchmark (DO NOT MODIFY)
runtimes = timeit.repeat(workload, number=1, repeat=3, setup=setup)
print(f"Mean: {statistics.mean(runtimes):.6f}")
print(f"Std Dev: {statistics.stdev(runtimes):.6f}")

